import { WechatyBuilder, log, ScanStatus} from 'wechaty';
import qrcodeTerminal from 'qrcode-terminal';
import OpenAI from 'openai';
import dotenv from 'dotenv';
import fetch from 'node-fetch';
import fs from 'fs';
import path from 'path';
import ffmpeg from 'fluent-ffmpeg';
import { FileBox } from 'file-box';

dotenv.config();

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const wechaty = WechatyBuilder.build();
const allowedContacts = ['å¶å»ºå¹³', 'åº†èŠ', 'EvieðŸ’«','Yale']; // Specify allowed contacts here
const userConversations = {}; // Store user conversations here

async function getChatGPTReply(user, message) {
  try {
    // Initialize conversation for the user if not already present
    if (!userConversations[user]) {
      userConversations[user] = [{ role: "system", content: "You are a helpful assistant." }];
    }

    // Add the new user message to the conversation
    userConversations[user].push({ role: "user", content: message });

    const completion = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: userConversations[user],
    });

    const reply = completion.choices[0].message.content.trim();

    // Add the assistant's reply to the conversation
    userConversations[user].push({ role: "assistant", content: reply });

    return reply;
  } catch (error) {
    log.error('ChatGPT API request failed:', error);
    return "Sorry, I couldn't generate a response at this time.";
  }
}

async function generateSpeech(text, outputFile) {
  try {
    const response = await fetch('https://api.openai.com/v1/audio/speech', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: "tts-1",
        input: text,
        voice: "alloy",
        response_format: "mp3",
      }),
    });

    if (!response.ok) {
      throw new Error(`TTS API request failed: ${response.statusText}`);
    }

    const buffer = await response.buffer();
    fs.writeFileSync(outputFile, buffer);
    return outputFile;
  } catch (error) {
    log.error('TTS API request failed:', error);
    return null;
  }
}

async function convertToSilk(inputFile, outputFile) {
  return new Promise((resolve, reject) => {
    ffmpeg(inputFile)
      .toFormat('silk')
      .on('end', () => resolve(outputFile))
      .on('error', (err) => reject(err))
      .save(outputFile);
  });
}

function onScan(qrcode, status) {
  if (status === ScanStatus.Waiting || status === ScanStatus.Timeout) {
    qrcodeTerminal.generate(qrcode, { small: true });
    console.log(`Scan QR Code to login: ${status}\nhttps://wechaty.js.org/qrcode/${encodeURIComponent(qrcode)}`);
  } else {
    log.info('StarterBot', 'onScan: %s(%s)', ScanStatus[status], status);
  }
}

function onLogin(user) {
  log.info('StarterBot', '%s logged in', user);
}

function onLogout(user) {
  log.info('StarterBot', '%s logged out', user);
}

async function handleVoiceMessage(msg) {
  const text = "This is a response generated by OpenAI's TTS service.";
  const mp3File = path.join(__dirname, 'response.mp3');
  const silkFile = path.join(__dirname, 'response.silk');
  const speechFile = await generateSpeech(text, mp3File);

  if (speechFile) {
    try {
      await convertToSilk(mp3File, silkFile);
      const fileBox = FileBox.fromFile(silkFile);
      await msg.say(fileBox);
    } catch (error) {
      log.error('Audio conversion failed:', error);
      msg.say("Sorry, I couldn't generate a speech response at this time.");
    } finally {
      fs.unlinkSync(mp3File);
      fs.unlinkSync(silkFile);
    }
  } else {
    msg.say("Sorry, I couldn't generate a speech response at this time.");
  }
}

async function onMessage(msg) {
  log.info('StarterBot', 'Message: %s', msg);

  if (msg.self()) {
    return;
  }

  const from = msg.from();
  const contactName = from.name();
  const room = msg.room();

  // Ignore messages from group chats
  if (room) {
    log.info('Message', `Ignoring message from room: ${room.topic()}`);
    return;
  }

  if (allowedContacts.includes(contactName)) {
    switch (msg.type()) {
      case wechaty.Message.Type.Text:
        const text = msg.text();
        log.info('Message', `Contact: ${from.name()} Text: ${text}`);
        if (text === "ç»“æŸå¯¹è¯" && userConversations[contactName]) {
          userConversations[contactName] = [{ role: "system", content: "You are a helpful assistant." }];
          log.info('Message', `Chat already finished`);
          msg.say("å¯¹è¯å·²ç»“æŸ");
        } else {
          const reply = await getChatGPTReply(contactName, text);
          msg.say(reply);
        }
        break;
      case wechaty.Message.Type.Audio:
        await handleVoiceMessage(msg);
        break;
      // Add more cases to handle other message types if needed
      default:
        log.info('Message', `Unhandled message type: ${msg.type()}`);
        break;
    }
  } else {
    log.info('StarterBot', `Message from ${contactName} is ignored.`);
  }
}

wechaty
  .on('scan', onScan)
  .on('login', onLogin)
  .on('logout', onLogout)
  .on('message', onMessage);

wechaty.start()
  .catch(e => console.error('Bot start failed:', e));
